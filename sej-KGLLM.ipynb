{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3wMU0__8e9M"
   },
   "source": [
    "This code is inspired by several tutorials from llama-index and langchain. In order to execute this code entirely you need to install and run a Nebula GraphStore on your machine so this code can store and request a KnowledgeGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:39:06.772813Z",
     "start_time": "2024-06-16T16:39:02.077159Z"
    },
    "id": "xwHfgf298e9P"
   },
   "outputs": [],
   "source": [
    "# First we install everything we need\n",
    "!python3 -m pip install llama-index-llms-openai llama-index-llms-langchain llama-index-llms-fireworks llama-index-graph-stores-nebula langchain flask bs4 langchain.community llama-index requests langchain-openai\n",
    "# Second we import everything\n",
    "from llama_index.core import KnowledgeGraphIndex, SimpleDirectoryReader\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "from llama_index.core import Settings\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import StructuredTool, Tool\n",
    "from langchain.chains import NebulaGraphQAChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "from langchain_community.graphs import NebulaGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "import requests\n",
    "from flask import Flask, request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkFweeH-8e9Q"
   },
   "source": [
    "First we download the wikipedia page for Search Engine Optimization and store it in the data directory. You can skip this step and put any files you want in the data directory to change the expertise of the agent. The code will create the data directory if it does not exists. If you skip this step make sure to create the directory before adding documents to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T16:41:45.655583Z",
     "start_time": "2024-06-16T16:41:44.535350Z"
    },
    "id": "olwbNmuL8e9S"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Getting the Wikipedia page\n",
    "res = requests.get(\"https://en.wikipedia.org/wiki/Search_engine_optimization\")\n",
    "if res.status_code == requests.codes.OK:\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    if not os.path.exists('./data'):\n",
    "        os.makedirs('./data')\n",
    "    with open(\"./data/wikipedia_seo.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(soup.body.get_text())\n",
    "else:\n",
    "    print(res.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByhdQYO38e9S"
   },
   "source": [
    "Before loading documents into the knowledge graph you must have an instance of Nebula graph store running. Visit https://docs.nebula-graph.io/3.6.0/ to see the different options (Docker or on premise). We strongly recommend avoiding Docker as while much simpler it is very much slower and can impact the result of the code. Once you have a datastore running, you should add a space called sej_graph_rag (or any other name) with the TAG entity (name string) and the EDGE TYPE relationship(relationship string). The command required to create the space tag and relationship are the following:\n",
    "\n",
    "CREATE SPACE sej_graph_rag(vid_type=FIXED_STRING(256));\n",
    "USE sej_graph_rag;\n",
    "CREATE TAG entity(name string);\n",
    "CREATE EDGE relationship(relationship string);\n",
    "\n",
    "(if you run into the error 'not enough host' while executing this command please run the following command: \n",
    "ADD HOSTS \"storaged0\":9779\n",
    ")\n",
    "\n",
    "Once this is done you can execute the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:13:54.766632442Z",
     "start_time": "2024-06-14T13:07:00.731823168Z"
    },
    "id": "IscwFbMx8e9T"
   },
   "outputs": [],
   "source": [
    "os.environ[\"NEBULA_USER\"] = \"root\" # Put your nebula username\n",
    "os.environ[\"NEBULA_PASSWORD\"] = \"nebula\"  # Put your nebula password\n",
    "os.environ[\"NEBULA_ADDRESS\"] = \"localhost:9669\"  # Change if you didn't use the defaults\n",
    "\n",
    "space_name = \"sej_graph_rag\" # Change it if you used another name for the space\n",
    "edge_types, rel_prop_names = [\"relationship\"], [\"relationship\"]\n",
    "tags = [\"entity\"]\n",
    "\n",
    "graph_store = NebulaGraphStore(\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Fill with your OpenAI API key\n",
    "# Create the LLM using the GPT-4 model\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "# Read the documents in the ./data directory\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "# This could take some time depending on the number of documents\n",
    "index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=10,\n",
    "    space_name=space_name,\n",
    "    edge_types=edge_types,\n",
    "    rel_prop_names=rel_prop_names,\n",
    "    tags=tags,\n",
    "    include_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBkdSRNL8e9U"
   },
   "source": [
    "Right now your knowledge graph is populated. You can go check using the nebula console to query your graph. Next step is creating the tools for our chatbot. The first one will use the Knowledge Graph as a database to answer the questions we have. We create a Langchain NebulaGraphQAChain in order to do so. We slightly modify the prompt given to the tools to reflect the schema used in the KG. We also help the LLM know what to do when it receives the result of its query to the Nebula Graph Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:14:00.180346185Z",
     "start_time": "2024-06-14T13:14:00.069512423Z"
    },
    "id": "3qcb3l748e9U"
   },
   "outputs": [],
   "source": [
    "graph = NebulaGraph(\n",
    "    space=\"sej_graph_rag\",\n",
    "    username=\"root\", # Your Nebula username\n",
    "    password=\"nebula\", # Your Nebula password\n",
    "    address=\"localhost\", # Change if necessary\n",
    "    port=9669, # Change if necessary\n",
    "    session_pool_size=30,\n",
    ")\n",
    "graph.refresh_schema()\n",
    "\n",
    "ngql_prompt = PromptTemplate(input_variables=['question', 'schema'],\n",
    "                             template=\"\"\"Task:Generate NebulaGraph Cypher statement to query a graph database.\n",
    "\n",
    "                             Instructions:\n",
    "\n",
    "                             First, generate cypher then convert it to NebulaGraph Cypher dialect(rather than standard):\n",
    "                             1. it requires explicit label specification only when referring to node properties: v.`entity`.name\n",
    "                             2. note explicit label specification is not needed for edge properties, so it's e.name instead of e.`relationship`.name\n",
    "                             3. it uses double equals sign for comparison: `==` rather than `=`\n",
    "                             For instance:\n",
    "                             ```diff\n",
    "                             < MATCH (p:entity)-[e:relationship]->(m:entity) WHERE p.name = 'The Godfather II'\n",
    "                             < RETURN p.name, e.year, m.name;\n",
    "                             ---\n",
    "                             > MATCH (p:`entity`)-[e:relationship]->(m:`entity`) WHERE lower(p.`entity`.`name`) == lower('The Godfather II')\n",
    "                             > RETURN p.`entity`.`name`, e.relationship, m.`entity`.`name`;\n",
    "                             ```\n",
    "\n",
    "                             Use only the provided relationship types and properties in the schema.\n",
    "                             Do not use any other relationship types or properties that are not provided.\n",
    "                             Schema:\n",
    "                             {schema}\n",
    "                             Note: Do not include any explanations or apologies in your responses.\n",
    "                             Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "                             Do not include any text except the generated Cypher statement.\n",
    "\n",
    "                             The question is:\n",
    "                             {question}\"\"\")\n",
    "qa_prompt = PromptTemplate(input_variables=['context', 'question'],\n",
    "                           template=\"\"\"You are an assistant that helps to form nice and human understandable answers.\n",
    "                           The information part contains the provided information that you must use to construct an answer.\n",
    "                           The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
    "                           Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
    "                           Here is an example:\n",
    "\n",
    "                           Question: Who is Toto?\n",
    "                           Context:['p.entity.name': ['Toto', 'Toto', 'Toto'], 'e.relationship': ['Is', 'Has won', 'Knows'], 'm.entity.name': ['Pro surfer', 'The Olympics', 'Kelly Slater']]\n",
    "                           Helpful Answer: Toto is a pro surfer that has won the olympics. He also knows Kelly Slater.\n",
    "\n",
    "                           Follow this example when generating answers. Use has many meaningful triplets as possible with a maximum of 7.\n",
    "                           If the provided information is empty, say that you don't know the answer.\n",
    "                           Information:\n",
    "                           {context}\n",
    "\n",
    "                           Question: {question}\n",
    "                           Helpful Answer:\"\"\")\n",
    "\n",
    "\n",
    "nebula_chain = NebulaGraphQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0),\n",
    "    qa_prompt=qa_prompt,\n",
    "    ngql_prompt=ngql_prompt,\n",
    "    graph=graph, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUWP1zbb8e9V"
   },
   "source": [
    "We now add to functions to interact with the Babbar API, one to get the metrics of the host and the other to get the backlinks ordered in decreasing BabbarAuthorityScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:14:02.267432376Z",
     "start_time": "2024-06-14T13:14:02.264371944Z"
    },
    "id": "lWT_8vtI8e9W"
   },
   "outputs": [],
   "source": [
    "BABBAR_URL = \"https://www.babbar.tech/api/host/overview/main?api_token=\"\n",
    "BABBAR_LINKS = \"https://www.babbar.tech/api/host/backlinks/url?api_token=\"\n",
    "BABBAR_KEY = \"\" # Fill with your Babbar API key\n",
    "\n",
    "\n",
    "def get_babbar_metrics(url: str) -> dict[str: float]:\n",
    "    \"\"\"Sends a POST request to the BABBAR API to get metrics for the url\"\"\"\n",
    "    res = requests.post(BABBAR_URL + BABBAR_KEY,\n",
    "                        json={'host': url})\n",
    "    if res.status_code == requests.codes.OK:\n",
    "        return res.json()\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def get_host_backlinks(url: str) -> dict:\n",
    "    \"\"\"Sends a POST request to the BABBAR API to get the backlinks for the url\n",
    "    : Arguments :\n",
    "    : url : (str) : The URL for the requests\"\"\"\n",
    "\n",
    "    res = requests.post(BABBAR_LINKS + BABBAR_KEY,\n",
    "                        json={'host': url, 'limit': 10, 'type': 'babbarAuthorityScore'})\n",
    "    if res.status_code == requests.codes.OK:\n",
    "        return res.json()\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsfU2H098e9W"
   },
   "source": [
    "Know we can create the \"tools\" our agent will use to answer our questions and initialize the agent. We will give it a memory as it will make the interactions easier. You will notice that the StructuredTool.from_function don't have a description. Langchain will use the description of the functions as the description of the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:14:03.960716269Z",
     "start_time": "2024-06-14T13:14:03.926649343Z"
    },
    "id": "JVJRr5328e9W"
   },
   "outputs": [],
   "source": [
    "tools = [StructuredTool.from_function(get_babbar_metrics),\n",
    "        StructuredTool.from_function(get_host_backlinks),\n",
    "        Tool.from_function(\n",
    "             func=nebula_chain.run,\n",
    "             name=\"Global Knowledge Base\",\n",
    "             description=\"Always use this tool first to get information about anything. If it cannot answer try something else.\"\n",
    "         ),\n",
    "         ]\n",
    "am = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "agent = initialize_agent(tools, ChatOpenAI(temperature=0.0, model_name=\"gpt-4\"),\n",
    "                         agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "                         verbose=True, memory=am)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At5vvtW88e9W"
   },
   "source": [
    "Now that we have an agent ready to answer questions we will build a flask interface with one route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:15:57.746696342Z",
     "start_time": "2024-06-14T13:14:05.564284326Z"
    },
    "id": "ckgafHSy8e9W"
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "app.config['FLASK_SECRET'] = 'A big secret'\n",
    "\n",
    "\n",
    "def get_answer(question: str):\n",
    "    \"\"\"\n",
    "    : Arguments :\n",
    "    : question : The question we want an answer to\"\"\"\n",
    "\n",
    "    result = agent.run(f\"Using a tool first, answer the following question: {question}\\nIf the tool doesn't know the answer try something else.\\nPlease answer in the same language the question was asked in.\")\n",
    "    return {\"question\": question, \"answer\": result}\n",
    "\n",
    "@app.route(\"/answer\", methods=[\"POST\"])\n",
    "def answer_query():\n",
    "    data = request.json\n",
    "    return get_answer(data.get(\"query\"))\n",
    "\n",
    "app.run(port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X11gEmG18e9W"
   },
   "source": [
    "You can now make POST requests to http://localhost:5000/answer. Make sure to send your query in JSON format with key \"query\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
